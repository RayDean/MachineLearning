【火炉炼AI】机器学习028-五分钟教你打造机器学习流水线
-

(本文所使用的Python库和版本号: Python 3.6, Numpy 1.14, scikit-learn 0.19, matplotlib 2.2 )

现在的社会工业化大生产离不开流水线作业，有了流水线，我们可以轻松的制造出成千上万相同的产品，而且所需要的价格成本极大地下降，所以说，流水线操作，使得工业化生产水平极大的提高。

那么有没有可能将这种流水线的处理思想转移到机器学习领域了？我们可不可以将数据清洗-数据规整-数据处理-特征选择-监督学习-模型评估等一整套流程做成一条机器学习的流水线了？如果可以，那就能极大的节省我们打造一个AI模型的时间，极大的提高构建优秀AI的效率。

在此，【火炉炼AI】可以十分肯定的告诉你，这是可以的，而且打造这种机器学习流水线非常方便，只需要短短的五分钟即可。

<br/>

## 1. 流水线第一步：准备数据集

数据集在本项目中反而不是很重要，故而我们用sklearn自带模块samples_generator生成一些示例数据即可。虽然numpy也在random模块中有随机产生数据集的函数，但是numpy比较适合用于产生一些简单的抽样数据。而sklearn中的datasets类却可以用来产生适合机器学习模型的数据集。

sklearn的datasets中常用的API有：

1) 用make_regression 生成回归模型的数据

2) 用make_hastie_10_2，make_classification或者make_multilabel_classification生成分类模型数据

3) 用make_blobs生成聚类模型数据

4) 用make_gaussian_quantiles生成分组多维正态分布的数据

```Python
# 准备数据集
from sklearn.datasets import samples_generator
# 使用这个函数产生示例数据
X,y=samples_generator.make_classification(n_informative=4,
                                          n_features=20,
                                          n_redundant=0,
                                          random_state=5)
# 产生一个分类数据集，包含有100个样本，20个features，2个类别，没有冗余特征。
# print(X.shape) # (100, 20)
# print(y.shape)  # (100,)
# print(X[:3]) # 查看没有问题
```

<br/>

## 2. 流水线第二步：构建特征选择器

在数据集准备完成之后，需要提取数据集中最重要的几个特征，即对我们的分类结果影响最大的几个主要特征，这样做可以减小模型的复杂程度，同时还能保持模型的预测精准度。sklearn框架也为我们准备好了特征选择函数SelectKBest()，我们只需要指定要选择的特征数K即可。如下为代码，非常简单。

```Python
# 建立特征选择器
from sklearn.feature_selection import SelectKBest, f_regression
feature_selector=SelectKBest(f_regression,k=10) 
# 一共20个特征向量，我们从中选择最重要的10个特征向量
```

<br/>

## 3. 流水线第三步：构建分类器

下一步，我们就需要构建分类器模型，前面在我的文章中讲到了很多分类器算法，比如SVM，随机森林，朴素贝叶斯等，此处我们构建一个简单的随机森林分类器作为例子。

```Python
# 建立分类器
from sklearn.ensemble import RandomForestClassifier
classifier=RandomForestClassifier(n_estimators=50,max_depth=4)
# 此处构建随机森林分类器作为例子，参数随便指定
```

<br/>

## 4. 流水线第四步：组装完整流水线

上面的几个步骤相当于建立各种产品处理模块，这一步我们就需要将这些模块组装起来，构建成一个可以完整运行的机器学习流水线。代码很简单，如下所示。

```Python
# 第四步：组装完整流水线
from sklearn.pipeline import Pipeline
pipeline=Pipeline([('selector',feature_selector),
                   ('rf_classifier',classifier)])
# 修改流水线中参数设置
# 假如我们希望特征选择器不是选择10个特征，而是5个特征，
# 同时分类器中的参数n_estimators也要修改一下，可以采用：
pipeline.set_params(selector__k=5,
                    rf_classifier__n_estimators=25)
```

**-------------------------------------输---------出--------------------------------**

Pipeline(memory=None,
 steps=[('selector', SelectKBest(k=5, score_func=<function f_regression at 0x000000001891B7B8>)), ('rf_classifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
max_depth=4, max_features='auto', max_leaf_nodes=None,
min_impurity_decrease=0.0, min...n_jobs=1,
oob_score=False, random_state=None, verbose=0,
warm_start=False))])

**--------------------------------------------完-------------------------------------**

从上面的输出中可以看出，这个流水线中只有两个模块，一个是特征选择器selector，另外一个是分类器rf_classifier，各自分别的参数位于后面。

对于模型，最终都是要用数据集进行训练，并且用训练好的模型来对新样本做出预测。如下为代码：

```Python
# 将数据输入到流水线中
pipeline.fit(X,y) # 对流水线进行训练

predict_y=pipeline.predict(X) #用训练好的流水线预测样本
# print(predict_y)

# 评估该流水线的模型性能
print('pipeline model score: {:.3f}'.format(pipeline.score(X,y)))
```

**-------------------------------------输---------出--------------------------------**

pipeline model score: 0.960

**--------------------------------------------完-------------------------------------**

这个流水线模型在训练集上的得分为0.960，貌似性能还不错。

上面我们构建了一个特征选择器，但是怎么知道哪些特征被选择，哪些陪抛弃了？如下代码：

```Python
# 查看特征选择器选择的特征：
feature_status=pipeline.named_steps['selector'].get_support()
# get_support()会返回true/false，如果支持该feature，则为true.
selected_features=[]
for count,item in enumerate(feature_status):
    if item: selected_features.append(count)
print('selected features by pipeline, (0-indexed): \n{}'.format(
        selected_features))
```

**-------------------------------------输---------出--------------------------------**

selected features by pipeline, (0-indexed): 
[5, 9, 10, 11, 15]

**--------------------------------------------完-------------------------------------**

由此可以看出，流水线自动选择了五个特征（我们前面指定了k=5),这最重要的五个特征分别是标号为5,9,10,11,15的特征。


**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#小\*\*\*\*\*\*\*\*\*\*结\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**

**1，打造机器学习流水线非常简单，只需要先构建机器学习的基本模块即可，然后将这些模块组装起来。**

**2，前面的特征提取器选择最重要特征的过程是先进行单变量统计测试，然后从特征向量中抽取最优秀的特征。这种测试之后，向量空间中的每个特征将有一个评价分数，基于这些评价分数，选择最好的K个特征，一旦抽取出K个特征，一旦K维的特征向量形成，就可以用这个特征向量用于分类器的输入训练数据。**

**3，打造这种流水线的优势有很多，可以简单快速的构建机器学习模型，可以方便的提取最重要的K个特征向量，可以快速的评估构建的模型，正所谓，这是个快速进行AI模型构建的必备良器。**

**4，上面的流水线只是整合了特征选择，分类器部分，唯一让人遗憾的是，怎么把数据处理，数据清洗这部分也整合到流水线中？暂时我还没有找到这部分内容，有哪位朋友找到了这种方法，请联系我啊。。。**
**\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#**



<br/>

注：本部分代码已经全部上传到（[**我的github**](https://github.com/RayDean/MachineLearning)）上，欢迎下载。

参考资料:

1, Python机器学习经典实例，Prateek Joshi著，陶俊杰，陈小莉译